{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical,plot_model\n",
    "\n",
    "from keras.models import Input,Model,Sequential\n",
    "from keras.layers import LSTM,Embedding,Dropout,Activation,Reshape,Dense,GRU,Add,Flatten,concatenate\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('updated_combined.csv')#generated from Fake News stanford.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>stance_cat</th>\n",
       "      <th>stance_base</th>\n",
       "      <th>jaccard_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>police find mass graves least 15 bodies near m...</td>\n",
       "      <td>712</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>danny boyle directing untitled film seth rogen...</td>\n",
       "      <td>3</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>hundreds palestinians flee floods gaza israel ...</td>\n",
       "      <td>158</td>\n",
       "      <td>agree</td>\n",
       "      <td>hundreds palestinians evacuated homes sunday m...</td>\n",
       "      <td>0</td>\n",
       "      <td>related</td>\n",
       "      <td>79.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>christian bale passes role steve jobs actor re...</td>\n",
       "      <td>137</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>30 year old moscow resident hospitalized wound...</td>\n",
       "      <td>3</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>hbo apple talks 15 month apple tv streaming se...</td>\n",
       "      <td>1034</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>reuters canadian soldier shot canadian war mem...</td>\n",
       "      <td>3</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>spider burrowed tourist stomach chest</td>\n",
       "      <td>1923</td>\n",
       "      <td>disagree</td>\n",
       "      <td>fear arachnophobes story bunbury spiderman mig...</td>\n",
       "      <td>1</td>\n",
       "      <td>related</td>\n",
       "      <td>28.301887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body ID     Stance  \\\n",
       "0  police find mass graves least 15 bodies near m...      712  unrelated   \n",
       "1  hundreds palestinians flee floods gaza israel ...      158      agree   \n",
       "2  christian bale passes role steve jobs actor re...      137  unrelated   \n",
       "3  hbo apple talks 15 month apple tv streaming se...     1034  unrelated   \n",
       "4              spider burrowed tourist stomach chest     1923   disagree   \n",
       "\n",
       "                                         articleBody  stance_cat stance_base  \\\n",
       "0  danny boyle directing untitled film seth rogen...           3   unrelated   \n",
       "1  hundreds palestinians evacuated homes sunday m...           0     related   \n",
       "2  30 year old moscow resident hospitalized wound...           3   unrelated   \n",
       "3  reuters canadian soldier shot canadian war mem...           3   unrelated   \n",
       "4  fear arachnophobes story bunbury spiderman mig...           1     related   \n",
       "\n",
       "   jaccard_similarity  \n",
       "0            0.000000  \n",
       "1           79.545455  \n",
       "2            0.000000  \n",
       "3            0.000000  \n",
       "4           28.301887  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99944\n",
      "99944\n",
      "Vocabulary Length is 23226\n"
     ]
    }
   ],
   "source": [
    "corpus = np.r_[data['Headline'].values,data['articleBody'].values]\n",
    "print(49972*2)\n",
    "print(len(corpus)) # first 49972 contains the Headline and next 49972 contains the articleBody\n",
    "\n",
    "vocabulary = []\n",
    "for sentence in corpus:\n",
    "    vocabulary.extend(sentence.split(' '))\n",
    "\n",
    "vocabulary = list(set(vocabulary))\n",
    "vocab_length = len(vocabulary)\n",
    "print(\"Vocabulary Length is {0}\".format(vocab_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "max_nb_words = 24000\n",
    "EMBEDDING_DIM = 50\n",
    "MAX_SEQUENCE_LENGTH = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE - ONE HOT ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoded_docs_headline = [one_hot(sentence,vocab_length) for sentence in data.loc[:,'Headline'].tolist()]\n",
    "padded_docs_headline = pad_sequences(encoded_docs_headline,MAX_SEQUENCE_LENGTH,padding='post')\n",
    "\n",
    "encoded_docs_body = [one_hot(sentence,vocab_length) for sentence in data.loc[:,'articleBody'].tolist()]\n",
    "padded_docs_body = pad_sequences(encoded_docs_body,MAX_SEQUENCE_LENGTH,padding='post')\n",
    "\n",
    "\n",
    "labels = to_categorical(data.loc[:,'stance_cat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abhinav.jha\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "input_headline = Input(shape=[64],name='input_headline')\n",
    "embedding_headline = Embedding(vocab_length,50,input_length = MAX_SEQUENCE_LENGTH)(input_headline)\n",
    "dense_headline = Dense(16,activation='relu')(embedding_headline)\n",
    "\n",
    "input_body = Input(shape=[64],name='input_body')\n",
    "embedding_body = Embedding(vocab_length,50,input_length = MAX_SEQUENCE_LENGTH)(input_body)\n",
    "dense_body = Dense(16,activation='relu')(embedding_body)\n",
    "\n",
    "addition_layer = concatenate([dense_body,dense_headline])\n",
    "flatten = Flatten()(addition_layer)\n",
    "output = Dense(4,activation='sigmoid')(flatten)\n",
    "\n",
    "model_combined = Model(inputs=[input_headline,input_body],outputs=output)\n",
    "\n",
    "model_combined.compile(optimizer = 'adam',loss ='categorical_crossentropy',metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_body (InputLayer)         (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_headline (InputLayer)     (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 64, 50)       1161300     input_body[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 64, 50)       1161300     input_headline[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64, 16)       816         embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64, 16)       816         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 32)       0           dense_2[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            8196        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,332,428\n",
      "Trainable params: 2,332,428\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_combined.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs_headline_train = padded_docs_headline[:int(len(padded_docs_headline)*0.8),:]\n",
    "padded_docs_headline_test = padded_docs_headline[int(len(padded_docs_headline)*0.8):,:]\n",
    "\n",
    "padded_docs_body_train = padded_docs_body[:int(len(padded_docs_body)*0.8),:]\n",
    "padded_docs_body_test = padded_docs_body[int(len(padded_docs_body)*0.8):,:]\n",
    "\n",
    "labels_train = labels[:int(len(labels)*0.8),:]\n",
    "labels_test = labels[int(len(labels)*0.8):,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abhinav.jha\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\abhinav.jha\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 39977 samples, validate on 9995 samples\n",
      "Epoch 1/5\n",
      "39977/39977 [==============================] - 66s 2ms/step - loss: 0.5906 - accuracy: 0.7844 - val_loss: 0.4630 - val_accuracy: 0.8246\n",
      "Epoch 2/5\n",
      "39977/39977 [==============================] - 67s 2ms/step - loss: 0.4165 - accuracy: 0.8398 - val_loss: 0.4121 - val_accuracy: 0.8420\n",
      "Epoch 3/5\n",
      "39977/39977 [==============================] - 56s 1ms/step - loss: 0.3575 - accuracy: 0.8582 - val_loss: 0.3863 - val_accuracy: 0.8523\n",
      "Epoch 4/5\n",
      "39977/39977 [==============================] - 57s 1ms/step - loss: 0.3182 - accuracy: 0.8704 - val_loss: 0.3782 - val_accuracy: 0.8580\n",
      "Epoch 5/5\n",
      "39977/39977 [==============================] - 58s 1ms/step - loss: 0.2921 - accuracy: 0.8775 - val_loss: 0.3706 - val_accuracy: 0.8612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x17c5ddfe348>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_combined.fit([padded_docs_headline_train,padded_docs_body_train],labels_train,epochs=5,verbose=1,validation_data=([padded_docs_headline_test,padded_docs_body_test],labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi- Directional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = \"D:\\twitter_sentiments\\emotions_classification\\glove\"\n",
    "def setup_embedding_index():\n",
    "    embedding_index=dict()\n",
    "    f = open(\"D:\\\\MyPython_Scripts\\\\emotions_classification\\\\glove\\\\glove.6B.50d.txt\",encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.array(values[1:],dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "    f.close()\n",
    "    return embedding_index\n",
    "embeddings_index = setup_embedding_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3255\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data.loc[:,'Headline'].values)\n",
    "vocab_headline_length = len(tokenizer.word_index)+1\n",
    "encoded_docs= tokenizer.texts_to_sequences(data.loc[:,'Headline'])\n",
    "padded_docs_headline = pad_sequences(encoded_docs, maxlen=16, padding='post')\n",
    "print(vocab_headline_length)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_headline = np.zeros((vocab_headline_length,EMBEDDING_DIM))\n",
    "# words = (list(word_index.keys()))[:max_nb_words]\n",
    "\n",
    "# for word,i in word_index.items():\n",
    "#     if i>=max_nb_words:\n",
    "#         continue\n",
    "#     embedding_vector = embedding_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix_headline[i] = embedding_vector\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_headline[i] = embedding_vector\n",
    "dims = len(embedding_matrix_headline[0])\n",
    "\n",
    "print(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23045\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data.loc[:,'articleBody'].values)\n",
    "vocab_body_length = len(tokenizer.word_index)+1\n",
    "encoded_docs= tokenizer.texts_to_sequences(data.loc[:,'Headline'])\n",
    "padded_docs_body = pad_sequences(encoded_docs, maxlen=48, padding='post')\n",
    "print(vocab_body_length)\n",
    "vocab_length = max(vocab_body_length,vocab_headline_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49972, 16)\n",
      "(49972, 48)\n"
     ]
    }
   ],
   "source": [
    "print(padded_docs_headline.shape)\n",
    "print(padded_docs_body.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.fit_on_texts(data.loc[:,'articleBody'].values)\n",
    "# encoded_docs= tokenizer.texts_to_sequences(data.loc[:,'articleBody'])\n",
    "\n",
    "# X_en = pad_sequences(encoded_docs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "# X_encoded = np.concatenate((X_encoded,X_en),axis=1)\n",
    "word_index = tokenizer.word_index\n",
    "# num_words = min(max_nb_words,len(word_index))\n",
    "# print('Number of words',num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23044\n"
     ]
    }
   ],
   "source": [
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_body = np.zeros((vocab_body_length,EMBEDDING_DIM))\n",
    "# words = (list(word_index.keys()))[:max_nb_words]\n",
    "\n",
    "# for word,i in word_index.items():\n",
    "#     if i>=max_nb_words:\n",
    "#         continue\n",
    "#     embedding_vector = embedding_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix_body[i] = embedding_vector\n",
    "# dims = len(embedding_matrix_body[0])\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_body[i] = embedding_vector\n",
    "dims = len(embedding_matrix_body[0])\n",
    "print(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abhinav.jha\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "input_headline = Input(shape=[16],name='input_headline')\n",
    "embedding_layer_headline = Embedding(vocab_headline_length,dims,weights=[embedding_matrix_headline],input_length = 16,trainable=False)(input_headline)\n",
    "\n",
    "# lstm_headline = LSTM(units=16)(embedding_headline)\n",
    "\n",
    "input_body = Input(shape=[48],name='input_body')\n",
    "embedding_layer_body = Embedding(vocab_body_length,dims,weights = [embedding_matrix_body],input_length=48,trainable = False)(input_body)\n",
    "# lstm_body = LSTM(units=16)(embedding_layer_body)\n",
    "\n",
    "addition_layer = concatenate([embedding_layer_headline,embedding_layer_body],axis=1)\n",
    "lstm = LSTM(units=64)(addition_layer)\n",
    "drop = Dropout(0.25)(lstm)\n",
    "dense = Dense(64,activation='relu')(drop)\n",
    "# flatten = Flatten()(addition_layer)\n",
    "\n",
    "output = Dense(4,activation='sigmoid')(dense)\n",
    "\n",
    "model_combined_lstm = Model(inputs=[input_headline,input_body],outputs=output)\n",
    "from keras.optimizers import SGD\n",
    "sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# model.compile(loss = \"categorical_crossentropy\", optimizer = opt)\n",
    "\n",
    "model_combined_lstm.compile(optimizer = sgd,loss ='categorical_crossentropy',metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_headline (InputLayer)     (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_body (InputLayer)         (None, 48)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 16, 50)       162750      input_headline[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 48, 50)       1152250     input_body[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 50)       0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 64)           29440       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           4160        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            260         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,348,860\n",
      "Trainable params: 33,860\n",
      "Non-trainable params: 1,315,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_combined_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs_headline_train = padded_docs_headline[:int(len(padded_docs_headline)*0.9),:]\n",
    "padded_docs_headline_test = padded_docs_headline[int(len(padded_docs_headline)*0.9):,:]\n",
    "\n",
    "padded_docs_body_train = padded_docs_body[:int(len(padded_docs_body)*0.9),:]\n",
    "padded_docs_body_test = padded_docs_body[int(len(padded_docs_body)*0.9):,:]\n",
    "\n",
    "labels = to_categorical(data.loc[:,'stance_cat'])\n",
    "\n",
    "labels_train = labels[:int(len(labels)*0.9),:]\n",
    "labels_test = labels[int(len(labels)*0.9):,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abhinav.jha\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\abhinav.jha\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 44974 samples, validate on 4998 samples\n",
      "Epoch 1/10\n",
      "44974/44974 [==============================] - 31s 692us/step - loss: 0.8073 - accuracy: 0.7307 - val_loss: 0.7986 - val_accuracy: 0.7321\n",
      "Epoch 2/10\n",
      "44974/44974 [==============================] - 31s 690us/step - loss: 0.7891 - accuracy: 0.7312 - val_loss: 0.7788 - val_accuracy: 0.7321\n",
      "Epoch 3/10\n",
      "44974/44974 [==============================] - 34s 758us/step - loss: 0.7770 - accuracy: 0.7312 - val_loss: 0.7777 - val_accuracy: 0.7321\n",
      "Epoch 4/10\n",
      "44974/44974 [==============================] - 33s 730us/step - loss: 0.7760 - accuracy: 0.7312 - val_loss: 0.7743 - val_accuracy: 0.7321\n",
      "Epoch 5/10\n",
      "44974/44974 [==============================] - 32s 719us/step - loss: 0.7650 - accuracy: 0.7312 - val_loss: 0.7614 - val_accuracy: 0.7321\n",
      "Epoch 6/10\n",
      "44974/44974 [==============================] - 32s 702us/step - loss: 0.7589 - accuracy: 0.7312 - val_loss: 0.7585 - val_accuracy: 0.7321\n",
      "Epoch 7/10\n",
      "44974/44974 [==============================] - 33s 743us/step - loss: 0.7540 - accuracy: 0.7312 - val_loss: 0.7547 - val_accuracy: 0.7321\n",
      "Epoch 8/10\n",
      "44974/44974 [==============================] - 35s 767us/step - loss: 0.7459 - accuracy: 0.7312 - val_loss: 0.7457 - val_accuracy: 0.7321\n",
      "Epoch 9/10\n",
      "44974/44974 [==============================] - 33s 736us/step - loss: 0.7396 - accuracy: 0.7312 - val_loss: 0.7512 - val_accuracy: 0.7321\n",
      "Epoch 10/10\n",
      "44974/44974 [==============================] - 32s 707us/step - loss: 0.7337 - accuracy: 0.7312 - val_loss: 0.7416 - val_accuracy: 0.7321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x18e99a9afc8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_combined_lstm.fit([padded_docs_headline_train,padded_docs_body_train],labels_train,epochs=10,shuffle=True,verbose=1,validation_data=([padded_docs_headline_test,padded_docs_body_test],labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train,y_train,x_val,y_val,params):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
